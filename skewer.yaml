title: Sharing a PostgreSQL database with distinct VANs
subtitle: |
  This tutorial demonstrates how to share a PostgreSQL database running on a namespace, without a Skupper site, with two distinct Virtual Application Networks.
overview: |
  In this tutorial, you will create two distinct Virtual Application Neworks (VANs) that enable communications across private and public clusters.
  
  You will then deploy a PostgreSQL database instance to a private cluster.
  The namespace where the database runs is not connected to any Virtual Application Network so we will use **_Attached Connectors_** to allow the
  database pods to be exposed by two distinct VANs, running in different namespaces within the same cluster.

  This will enable clients running alongside these separate Virtual Application Neworks to transparently access the database,
  without the need for additional networking setup (e.g. no vpn or sdn required).

  Here is an overview of the topology used by this example:

  <p align="center"><img src="images/diagram.png" width="640"/></p>
prerequisites: |
  @default@

  The basis for the demonstration is to depict the operation of a PostgreSQL database in a private cloud cluster, on a namespace that is not connected
  to any Virtual Application Network, but allows two internal namespaces to expose the database pods into the distinct VANs they are connected with.

  With that, we provide access to the database from clients resident on any cluster connected to these two separate VANs.
  
  As an example, the cluster deployment might be comprised of:

  * A private cloud cluster running on your local machine
  * Two public cloud clusters running on a public cloud provider

  While the detailed steps are not included here, this demonstration can alternatively be performed with five separate namespaces on a single cluster.
sites:
  company-a:
    title: Company A public cluster
    platform: kubernetes
    namespace: company-a
    env:
      KUBECONFIG: $PWD/kubeconfigs/company-a.config
  private-company-a:
    title: Company A private cluster
    platform: kubernetes
    namespace: private-company-a
    env:
      KUBECONFIG: $PWD/kubeconfigs/private-company-a.config
  company-b:
    title: Company B public cluster
    platform: kubernetes
    namespace: company-b
    env:
      KUBECONFIG: $PWD/kubeconfigs/company-b.config
  private-company-b:
    title: Company B private cluster
    platform: kubernetes
    namespace: private-company-b
    env:
      KUBECONFIG: $PWD/kubeconfigs/private-company-b.config
  internal:
    title: Internal namespace on private cluster
    platform: kubernetes
    namespace: internal
    env:
      KUBECONFIG: $PWD/kubeconfigs/internal.config
steps:
  - standard: platform/access_your_kubernetes_clusters
  - standard: platform/install_skupper_on_your_kubernetes_clusters
  - standard: platform/create_your_kubernetes_namespaces
  - title: Set up the demo
    preamble: |
      On your local machine, make a directory for this tutorial and clone the example repo:
    commands:
      company-a:
        - run: |
            cd ~/
            mkdir pg-demo
            cd pg-demo
            git clone https://github.com/fgiorgetti/skupper-example-postgresql-attached-connector.git
  - title: Create your sites
    preamble: |
      A Skupper _Site_ is a location where your application workloads
      are running. Sites are linked together to form a network for your
      application.

      Use the `kubectl apply` command to declaratively create sites in the kubernetes
      namespaces. This deploys the Skupper router. Then use `kubectl get site` to see
      the outcome.

        **Note:** If you are using Minikube, you need to [start minikube
        tunnel][minikube-tunnel] before you configure skupper.

        [minikube-tunnel]: https://skupper.io/start/minikube.html#running-minikube-tunnel

      The public site definitions for Company-A and Company-B, set `linkAccess: default`, because their respective private sites
      will establish a Skupper link to them, forming two separate VANs between the namespaces:

      * private-company-a -> company-a
      * private-company-b -> company-b

      This extra definition tells that the public sites accept incoming Skupper links from other sites, using the default ingress
      type for the target clusters (_route_ when using OpenShift or _loadbalancer_ otherwise).
    commands:
      company-a:
        - run: kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/company-a/site.yaml
      private-company-a:
        - run: kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/private-company-a/site.yaml
      company-b:
        - run: kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/company-b/site.yaml
      private-company-b:
        - run: kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/private-company-b/site.yaml
  - title: Link your sites
    preamble: |
      A Skupper _link_ is a channel for communication between two sites.
      Links serve as a transport for application connections and
      requests.

      Creating an AccessToken requires the creation of an AccessGrant first,
      on the public cluster, then we can consume the AccessGrant's status
      to write an AccessToken and apply it into the private cluster using `kubectl apply`.

      Since we are creating two distinct VANs, we need to create a separate AccessToken to each public cluster
      and apply them to the respective private cluster.

      **Note:** The link token is truly a *secret*.  Anyone who has the
      token can link to your site.  Make sure that only those you trust
      have access to it.
    commands:
      company-a:
        - run: kubectl wait --for=condition=ready site/company-a --timeout 300s
        - run: kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/company-a/accessgrant.yaml
        - run: kubectl wait --for=condition=ready accessgrant/company-a-grant --timeout 300s
        - run: |
            kubectl get accessgrant company-a-grant -o go-template-file=~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/token.template > ~/company-a.token
          apply: test  
        - run: |
            kubectl get accessgrant company-a-grant -o go-template-file=skupper-example-postgresql-attached-connector/kubernetes/token.template > ~/company-a.token
          apply: readme
      private-company-a:
        - run: kubectl apply -f ~/company-a.token
          output: |
            accesstoken.skupper.io/token-company-a-grant created
      company-b:
        - run: kubectl wait --for=condition=ready site/company-b --timeout 300s
        - run: kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/company-b/accessgrant.yaml
        - run: kubectl wait --for=condition=ready accessgrant/company-b-grant --timeout 300s
        - run: |
            kubectl get accessgrant company-b-grant -o go-template-file=~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/token.template > ~/company-b.token
          apply: test  
        - run: |
            kubectl get accessgrant company-b-grant -o go-template-file=skupper-example-postgresql-attached-connector/kubernetes/token.template > ~/company-b.token
          apply: readme
      private-company-b:
        - run: kubectl apply -f ~/company-b.token
          output: |
            accesstoken.skupper.io/token-company-b-grant created
    postamble: |
      If your terminal sessions are on different machines, you may need
      to use `scp` or a similar tool to transfer the token securely.  By
      default, tokens expire after a single use or 15 minutes after
      being issued.
  - title: Deploy the PostgreSQL service
    preamble: |
      After creating the application router network, deploy the PostgreSQL service.
      The **internal** namespace on the private cluster will be used to deploy the PostgreSQL server.

      The **Company A** and the **Company B** public and private clusters will be used to enable client
      communications to the server running on the **internal** namespace of the private cluster.
    commands:
      internal:
        - run: |
            kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/internal/deployment-postgresql-svc.yaml
          output: |
            secret/postgresql created
            deployment.apps/postgresql created
        - await_resource: deployment/postgresql
          apply: test
  - title: Expose the PostgreSQL on the Virtual Application Networks
    preamble: |
      Now that the PostgreSQL is running in the **internal** namespace of the private cluster, we need to expose it into
      both distinct Virtual Application Networks (VANs).

      Remember that the **internal** namespace is not connected to any VAN, so workloads running there cannot be exposed by other namespaces.

      Skupper V2 allows exposing resources from other namespaces, as long as the namespace where the workloads are
      running has an **_AttachedConnector_** definition, authorizing the source namespace, which is connected to a VAN, to bind it.

      The **_AttachedConnector_** must be defined at the namespace where the target workloads are running and it must specify:
      
      * The _selector_ (for target pods)
      * The target _port_ (of the respective pods), and,
      * The _siteNamespace_

      In this case, the _siteNamespace_ must be set to namespace which is connected to a VAN.

      In the authorized namespace (connected to a VAN), you must define an **_AttachedConnectorBinding_** resource that has:

      * The same name of the **_AttachedConnector_** defined on the target namespace (where workloads are running)
      * A _routingKey_ to be used by the participant VAN sites to access the database
      * The _connectorNamespace_ field set to the namespace where the workloads are running

      If all the settings above match, the site connected to a VAN, is allowed to expose the PostgreSQL pods running on the internal namespace.

      Since we are defining two separate VANs, we will need two separate pairs of Attached Connector + Attached Connector Binding.
      
      The **Attached Connectors** will be defined on the **internal** namespace of the private cluster, while the **Attached Connector Bindings**
      will be defined on the local namespaces connected to a VAN, therefore, one on **private-company-a** and the other on **private-company-b** namespaces.
    commands:
      internal:
        - run: |
            kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/internal/attached-connector-company-a.yaml
            kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/internal/attached-connector-company-b.yaml
          output: |
            attachedconnector.skupper.io/postgresql-company-a created
            attachedconnector.skupper.io/postgresql-company-b created
      private-company-a:
        - run: kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/private-company-a/attached-connector-binding.yaml
          output: |
            attachedconnectorbinding.skupper.io/postgresql-company-a created
      private-company-b:
        - run: kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/private-company-b/attached-connector-binding.yaml
          output: |
            attachedconnectorbinding.skupper.io/postgresql-company-b created
  - title: Making the PostgreSQL database accessible to the each VAN
    preamble: |
      In order to make the PostgreSQL database accessible to each VAN, we need to define a `Listener` on each of the Public Sites,
      which will produce a Kubernetes service on each respective namespace, connecting them with the database running on the **internal**
      namespace of the private cluster transparently.
    commands:
      company-a:
        - run: kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/company-a/listener.yaml
          output: |
            listener.skupper.io/postgresql created
        - run: kubectl wait --for=condition=ready listener/postgresql --timeout 300s
      company-b:
        - run: kubectl apply -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/company-b/listener.yaml
          output: |
            listener.skupper.io/postgresql created
        - run: kubectl wait --for=condition=ready listener/postgresql --timeout 300s
  - title: Create pod with PostgreSQL client utilities
    preamble: |
      Create a pod named `pg-shell` on each Public site. This pod will be used to
      communicate with the PostgreSQL database from the public clusters' namespaces.
    commands:
      company-a:
        - run: |
            kubectl run pg-shell --image quay.io/skupper/simple-pg \
            --env="PGUSER=postgres" \
            --env="PGPASSWORD=skupper" \
            --env="PGHOST=postgresql" \
            --command sleep infinity
          output: pod/pg-shell created
        - run: kubectl wait --for condition=ready --timeout 300s pod/pg-shell
          apply: test
      company-b:
        - run: |
            kubectl run pg-shell --image quay.io/skupper/simple-pg \
            --env="PGUSER=postgres" \
            --env="PGPASSWORD=skupper" \
            --env="PGHOST=postgresql" \
            --command sleep infinity
          output: pod/pg-shell created
        - run: kubectl wait --for condition=ready --timeout 300s pod/pg-shell
          apply: test
  - title: Create databases, a table and insert values
    preamble: |
      Now that we can access the PostgreSQL database from both VANs, let's create two databases called **company-a** and **company-b**,
      then create a table named **product**, in each, and load them with some data.
    commands:
      company-a:
        - run: |
            kubectl exec pg-shell -- createdb -e company-a
            kubectl exec -i pg-shell -- psql -d company-a < ~/pg-demo/skupper-example-postgresql-attached-connector/sql/company-a/table.sql
            kubectl exec -i pg-shell -- psql -d company-a < ~/pg-demo/skupper-example-postgresql-attached-connector/sql/company-a/data.sql
          output: |
            SELECT pg_catalog.set_config('search_path', '', false);
            CREATE DATABASE company-a;
            CREATE TABLE
            INSERT 0 1
            INSERT 0 1
            INSERT 0 1
            INSERT 0 1
      company-b:
        - run: |
            kubectl exec pg-shell -- createdb -e company-b
            kubectl exec -i pg-shell -- psql -d company-b < ~/pg-demo/skupper-example-postgresql-attached-connector/sql/company-b/table.sql
            kubectl exec -i pg-shell -- psql -d company-b < ~/pg-demo/skupper-example-postgresql-attached-connector/sql/company-b/data.sql
          output: |
            SELECT pg_catalog.set_config('search_path', '', false);
            CREATE DATABASE company-b;
            CREATE TABLE
            INSERT 0 1
            INSERT 0 1
            INSERT 0 1
            INSERT 0 1
  - title: Access the product table from both sites
    preamble: |
      Now that data has been added, try to read them from both **company-a** and **company-b** sites.
    commands:
      company-a:
        - run: echo "SELECT * FROM product;" | kubectl exec -i pg-shell -- psql -d company-a
      company-b:
        - run: echo "SELECT * FROM product;" | kubectl exec -i pg-shell -- psql -d company-b
  - title: Cleaning up
    name: cleaning_up
    preamble: |
      Restore your cluster environment by returning the resources created in the demonstration. On each cluster, delete the 
      demo resources and the virtual application Network.
    numbered: false
    commands:
      company-a:
        - run: kubectl delete pod pg-shell --now
        - run: kubectl delete -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/company-a/
      private-company-a:
        - run: kubectl delete pod pg-shell --now
        - run: kubectl delete -f ~/company-a.token -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/company-a/
      company-b:
        - run: kubectl delete pod pg-shell --now
        - run: kubectl delete -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/company-b/
      private-company-b:
        - run: kubectl delete pod pg-shell --now
        - run: kubectl delete -f ~/company-b.token -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/company-b/
      internal:
        - run: kubectl delete -f ~/pg-demo/skupper-example-postgresql-attached-connector/kubernetes/internal/
summary: |
  Through this example, we demonstrated how Skupper enables secure access to a PostgreSQL database hosted in a
  private Kubernetes cluster, on a namespace that is not connected to any Virtual Application Network, without exposing it to the public internet.

  By deploying Skupper in each namespace, we established two **Virtual Application Networks** (VANs), which allowed
  the PostgreSQL service to be securely shared across the two distinct VANs.
  
  The **AttachedConnector** and the **AttachedConnectorBinding** provided by Skupper, allows you to set a granular definition on which
  workloads can be bound by Skupper sites running on different local namespaces. Namespaces that are not explicitly allowed through **AttachedConnectors**,
  cannot bind pods and cannot expose them into the VAN.

  It is important to emphasize that the database was made available exclusively within the VANs authorized by the **Attached Connectors**,
  where matching **Attached Connector Bindings** have been defined. With that, applications in the public clusters, are able to access it seamlessly,
  as if it were running locally in their own namespaces.

  This approach not only simplifies multi-cluster communication but also preserves strict network boundaries, eliminating the need for complex VPNs or
  firewall changes.
